{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbf7431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, lfilter\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import statistics\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from math import log\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sktime as sktime\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "from sktime.alignment.dtw_python import AlignerDTWfromDist\n",
    "from sktime.dists_kernels.scipy_dist import ScipyDist\n",
    "from sktime.dists_kernels.compose_from_align import DistFromAligner\n",
    "import os\n",
    "import sys\n",
    "%matplotlib inline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.classification.hybrid import HIVECOTEV2\n",
    "from sktime.classification.dictionary_based import MUSE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48982a5",
   "metadata": {},
   "source": [
    "### Load normal and abnormal datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f3947",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_normal_df = pd.read_pickle('normal_EMO_pain.pkl')\n",
    "final_abnormal_df = pd.read_pickle('abnormal_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c129a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_abnormal_df = final_abnormal_df.drop(columns=['subject_number','activity'])\n",
    "final_abnormal_df.rename(columns={'new_subject_number': 'Subject'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_indices = (final_abnormal_df['Subject']).unique()\n",
    "abnormal_no = (final_abnormal_df['Subject']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff6733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path of the saved pkl file\n",
    "file_path = 'final_bins_67.pkl'\n",
    "\n",
    "# Load the dictionary from the pkl file\n",
    "with open(file_path, 'rb') as file:\n",
    "    final_numbers_dictionary = pickle.load(file)\n",
    "# Now, 'loaded_dictionary' contains the dictionary loaded from the pkl file\n",
    "display(\"Dictionary loaded from pickle file:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_keys = final_numbers_dictionary.keys()\n",
    "keys_list = list(dictionary_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091899d",
   "metadata": {},
   "source": [
    "### Function to find the length of the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to z-normalize a column\n",
    "def z_normalize(column):\n",
    "    mean = column.mean()\n",
    "    std = column.std()\n",
    "    z_normalized = (column - mean) / std\n",
    "    return z_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cef1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_length_finding(df):\n",
    "    train_1_abnormal = df.copy()\n",
    "    train_1_ab_sub = (train_1_abnormal['Subject'].unique()).tolist()\n",
    "    row_count = []\n",
    "    for i in train_1_ab_sub:\n",
    "        train_1_abnormal_new = train_1_abnormal[(train_1_abnormal['Subject']== i)]\n",
    "        no_rows = train_1_abnormal_new.shape[0]\n",
    "        row_count.append(no_rows)\n",
    "    display(\"Avg length:\",statistics.mean(row_count))\n",
    "    avg_len_ts = int(statistics.mean(row_count))\n",
    "    return(avg_len_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_normal_df(df,desired_length):\n",
    "    train_2_normal = df.copy()\n",
    "    df_train_3 = pd.DataFrame(columns = train_2_normal.columns)\n",
    "    num_dim = df_train_3.shape[1]\n",
    "    display(num_dim)\n",
    "    df_train_3.columns = range(len(df_train_3.columns))\n",
    "    train_2_sub = (train_2_normal['Subject'].unique()).tolist()\n",
    "    m = 0\n",
    "    for i in train_2_sub:\n",
    "        train_2_normal_new = train_2_normal[(train_2_normal['Subject']== i)]\n",
    "        df_row_count = train_2_normal_new.shape[0]\n",
    "        if df_row_count < desired_length:\n",
    "            rows_to_add = desired_length - df_row_count\n",
    "            train_2_normal_new.tail()\n",
    "            last_row = train_2_normal_new.ffill().iloc[[-1]]  # Extract the last row\n",
    "            new_rows_df = pd.concat([last_row] * rows_to_add, ignore_index=True)\n",
    "            train_2_normal_initial = pd.concat([train_2_normal_new, new_rows_df], ignore_index=True)\n",
    "            train_2_normal_initial_no_sub = train_2_normal_initial.drop(columns='Subject')\n",
    "            train_2_normal_final = z_normalize(train_2_normal_initial_no_sub)\n",
    "            train_2_normal_final.insert(0, 'Subject', i)\n",
    "\n",
    "        else:\n",
    "            train_2_normal_initial = train_2_normal_new.iloc[:desired_length,:]\n",
    "            train_2_normal_initial_no_sub = train_2_normal_initial.drop(columns='Subject')\n",
    "            train_2_normal_final = z_normalize(train_2_normal_initial_no_sub)\n",
    "            train_2_normal_final.insert(0, 'Subject', i)\n",
    "            \n",
    "        train_2_normal_final.columns = range(len(train_2_normal_final.columns))\n",
    "        \n",
    "        df_train_3.at[m,0] = i\n",
    "        j = 0\n",
    "        while j < num_dim:\n",
    "            df_train_3.at[m,j] = train_2_normal_final[j]\n",
    "            \n",
    "            j += 1\n",
    "        m += 1    \n",
    "    df_train_3['class'] = 'normal'\n",
    "    df_normal_baseline = df_train_3.iloc[:,1:]\n",
    "    return(df_normal_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dataframe(df,desired_length):\n",
    "    train_1_abnormal = df.copy()\n",
    "    df_train_2 = pd.DataFrame(columns = train_1_abnormal.columns)\n",
    "    num_dim_ab = df_train_2.shape[1]\n",
    "\n",
    "    df_train_2.columns = range(len(df_train_2.columns))\n",
    " \n",
    "    train_1_ab_sub = (train_1_abnormal['Subject'].unique()).tolist()\n",
    "    m = 0\n",
    "    for i in train_1_ab_sub:\n",
    "        train_1_abnormal_new = train_1_abnormal[(train_1_abnormal['Subject']== i)]\n",
    "        df_row_count = train_1_abnormal_new.shape[0]\n",
    "        if df_row_count < desired_length:\n",
    "            rows_to_add = desired_length - df_row_count\n",
    "\n",
    "            last_row = train_1_abnormal_new.ffill().iloc[[-1]]  # Extract the last row\n",
    "            new_rows_df = pd.concat([last_row] * rows_to_add, ignore_index=True)\n",
    "            train_1_abnormal_initial = pd.concat([train_1_abnormal_new, new_rows_df], ignore_index=True)\n",
    "            train_1_abnormal_initial_no_sub = train_1_abnormal_initial.drop(columns='Subject')\n",
    "            train_1_abnormal_final = z_normalize(train_1_abnormal_initial_no_sub)\n",
    "            train_1_abnormal_final.insert(0, 'Subject', i)\n",
    "\n",
    "        else:\n",
    "            train_1_abnormal_initial = train_1_abnormal_new.iloc[:desired_length,:]\n",
    "            train_1_abnormal_initial_no_sub = train_1_abnormal_initial.drop(columns='Subject')\n",
    "            train_1_abnormal_final = z_normalize(train_1_abnormal_initial_no_sub)\n",
    "            train_1_abnormal_final.insert(0, 'Subject', i)\n",
    "        train_1_abnormal_final.columns = range(len(train_1_abnormal_final.columns))\n",
    "\n",
    "        df_train_2.at[m,0] = i\n",
    "        j = 0\n",
    "        while j < num_dim_ab:\n",
    "            df_train_2.at[m,j] = train_1_abnormal_final[j]\n",
    "            j += 1\n",
    "        m += 1    \n",
    "    df_train_2['class'] = 'abnormal'\n",
    "    df_final = df_train_2.iloc[:,1:]\n",
    "    return(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_classifier(df_class_1):\n",
    "    acc_mean_list = []\n",
    "    prec_mean_list = []\n",
    "    recall_mean_list = []\n",
    "    f1_mean_list = []\n",
    "\n",
    "    X = df_class_1.iloc[:,:-1]\n",
    "    y = df_class_1.iloc[:,-1]\n",
    "\n",
    "    for i in range(1, 31):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i, stratify=y)\n",
    "        \n",
    "        accuracy_dict = {} \n",
    "        precision_dict = {}\n",
    "        recall_dict = {}\n",
    "        f1_dict = {}\n",
    "        \n",
    "        comb = 1\n",
    "        for a in[2,4,6]:\n",
    "            for b in [4,8]:\n",
    "                clf = MUSE(window_inc=a,alphabet_size=b,use_first_order_differences=False, random_state=42)\n",
    "                clf.fit(X_train, y_train)\n",
    "                \n",
    "                # Predict using the final model\n",
    "                y_pred = clf.predict(X_test)\n",
    "\n",
    "                pos_label = 'abnormal'\n",
    "\n",
    "                acc = accuracy_score(y_test, y_pred)\n",
    "                prec = precision_score(y_test, y_pred, pos_label=pos_label)\n",
    "                recall = recall_score(y_test, y_pred, pos_label=pos_label)\n",
    "                f1 = f1_score(y_test, y_pred, pos_label=pos_label)\n",
    "                \n",
    "                accuracy_dict[(a, b)] = acc\n",
    "                precision_dict[(a, b)] = prec\n",
    "                recall_dict[(a, b)] = recall\n",
    "                f1_dict[(a, b)] = f1\n",
    "                \n",
    "                display(\"Completed comb\",comb)\n",
    "                comb += 1\n",
    "        \n",
    "        accuracy_max_dict = max(accuracy_dict.items(), key=lambda x: x[1])\n",
    "        display(\"Accuracy Dictionary:\",accuracy_dict)\n",
    "        accuracy_max = accuracy_max_dict[1]\n",
    "\n",
    "        max_a_b = accuracy_max_dict[0]\n",
    "        display(\"(a, b) combination for maximum accuracy:\",max_a_b)\n",
    "\n",
    "        display('Accuracy: %f' % accuracy_max)\n",
    "        acc_mean_list.append(accuracy_max)\n",
    "\n",
    "        precision_max = precision_dict[max_a_b]\n",
    "        display('Precision: %f' % precision_max)\n",
    "        prec_mean_list.append(precision_max)\n",
    "\n",
    "        recall_max = recall_dict[max_a_b]\n",
    "        display('Recall: %f' % recall_max)\n",
    "        recall_mean_list.append(recall_max)\n",
    "\n",
    "        f1_score_max = f1_dict[max_a_b]\n",
    "        display('f1_score: %f' % f1_score_max)\n",
    "        f1_mean_list.append(f1_score_max)\n",
    "\n",
    "        display(\"Completed\", i)\n",
    "\n",
    "    acc_mean = np.mean(acc_mean_list)\n",
    "    acc_std_dev = np.std(acc_mean_list)\n",
    "    display('Accuracy Mean: %f' % acc_mean)\n",
    "\n",
    "    prec_mean = np.mean(prec_mean_list)\n",
    "    prec_std_dev = np.std(prec_mean_list)\n",
    "\n",
    "    recall_mean = np.mean(recall_mean_list)\n",
    "    recall_std_dev = np.std(recall_mean_list)\n",
    "\n",
    "    f1_mean = np.mean(f1_mean_list)\n",
    "    f1_std_dev = np.std(f1_mean_list)\n",
    "    display('F1 Mean: %f' % f1_mean)\n",
    "\n",
    "    return (acc_mean, acc_std_dev, prec_mean, prec_std_dev, recall_mean, recall_std_dev, f1_mean, f1_std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfc6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary with combined values\n",
    "# Keys to combine\n",
    "key_to_combine1 = 'sample_numbers_list_0.0_to_0.1'\n",
    "key_to_combine2 = 'sample_numbers_list_0.1_to_0.2'\n",
    "\n",
    "# Create a new dictionary with combined values\n",
    "hb_dict = {'hb_key': final_numbers_dictionary[key_to_combine1] + final_numbers_dictionary[key_to_combine2]}\n",
    "display(hb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72627479",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier_dict_hb = {}\n",
    "key = 'hb_key'\n",
    "normal_sample_numbers = [] \n",
    "normal_sample_numbers.extend(hb_dict[key])\n",
    "normal_sample_length = len(normal_sample_numbers)\n",
    "display(normal_sample_length)\n",
    "\n",
    "if len(normal_sample_numbers) <= abnormal_no:\n",
    "    df_normal = final_normal_df[final_normal_df['Subject'].isin(normal_sample_numbers)]\n",
    "    display(df_normal.head(2))\n",
    "\n",
    "    random.seed(42)\n",
    "    abnormal_sample_numbers = random.sample(abnormal_indices.tolist(), normal_sample_length)\n",
    "    df_abnormal = final_abnormal_df[final_abnormal_df['Subject'].isin(abnormal_sample_numbers)]\n",
    "    display(df_abnormal.head(2))\n",
    "\n",
    "    desired_length_for_ts = time_series_length_finding(df_abnormal)\n",
    "\n",
    "    df_normal_preprocessed = preprocessing_normal_df(df_normal,desired_length_for_ts)\n",
    "    df_abnormal_preprocessed = preprocessing_dataframe(df_abnormal,desired_length_for_ts)\n",
    "\n",
    "    df_classifier = (pd.concat([df_normal_preprocessed,df_abnormal_preprocessed]).reset_index()).drop(columns='index')\n",
    "    acc_mean,acc_std_dev,prec_mean,prec_std_dev,recall_mean,recall_std_dev,f1_mean,f1_std_dev = ts_classifier(df_classifier)\n",
    "\n",
    "    classifier_dict_hb[key] = {'acc_mean': acc_mean,'acc_std_dev': acc_std_dev,'prec_mean': prec_mean,'prec_std_dev': prec_std_dev,'recall_mean': recall_mean,'recall_std_dev': recall_std_dev,'f1_mean': f1_mean,'f1_std_dev': f1_std_dev}\n",
    "\n",
    "else:\n",
    "    df_abnormal = final_abnormal_df\n",
    "\n",
    "    random.seed(42)\n",
    "    random_sample_numbers = random.sample(normal_sample_numbers, abnormal_no)\n",
    "    df_normal = final_normal_df[final_normal_df['Subject'].isin(random_sample_numbers)]\n",
    "    display(df_normal)\n",
    "\n",
    "    desired_length_for_ts = time_series_length_finding(df_abnormal)\n",
    "\n",
    "    df_normal_preprocessed = preprocessing_normal_df(df_normal,desired_length_for_ts)\n",
    "    df_abnormal_preprocessed = preprocessing_dataframe(df_abnormal,desired_length_for_ts)\n",
    "\n",
    "    df_classifier = (pd.concat([df_normal_preprocessed,df_abnormal_preprocessed]).reset_index()).drop(columns='index')\n",
    "    acc_mean,acc_std_dev,prec_mean,prec_std_dev,recall_mean,recall_std_dev,f1_mean,f1_std_dev = ts_classifier(df_classifier)\n",
    "\n",
    "    classifier_dict_hb[key] = {'acc_mean': acc_mean,'acc_std_dev': acc_std_dev,'prec_mean': prec_mean,'prec_std_dev': prec_std_dev,'recall_mean': recall_mean,'recall_std_dev': recall_std_dev,'f1_mean': f1_mean,'f1_std_dev': f1_std_dev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the file path where you want to save the pickle file\n",
    "pickle_file_path = 'hb_EMO_pain_MUSE_normalized.pickle'\n",
    "\n",
    "# Save the dictionary to a pickle file\n",
    "with open(pickle_file_path, 'wb') as file:\n",
    "    pickle.dump(classifier_dict_hb, file)\n",
    "\n",
    "print(f\"Dictionary saved to {pickle_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f4764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dict_hb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f046f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values = [value for sublist in final_numbers_dictionary.values() for value in sublist]\n",
    "healthy_dict = {'healthy_key': all_values}\n",
    "display(healthy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44a4eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier_dict_healthy = {}\n",
    "key = 'healthy_key'\n",
    "normal_sample_numbers = []\n",
    "normal_sample_numbers.extend(healthy_dict[key])\n",
    "normal_sample_length = len(normal_sample_numbers)\n",
    "display(normal_sample_length)\n",
    "\n",
    "if len(normal_sample_numbers) <= abnormal_no:\n",
    "    df_normal = final_normal_df[final_normal_df['Subject'].isin(normal_sample_numbers)]\n",
    "    display(df_normal.head())\n",
    "\n",
    "    random.seed(42)\n",
    "    abnormal_sample_numbers = random.sample(abnormal_indices.tolist(), normal_sample_length)\n",
    "    df_abnormal = final_abnormal_df[final_abnormal_df['Subject'].isin(abnormal_sample_numbers)]\n",
    "    display(df_abnormal.head())\n",
    "\n",
    "    desired_length_for_ts = time_series_length_finding(df_abnormal)\n",
    "\n",
    "    df_normal_preprocessed = preprocessing_normal_df(df_normal,desired_length_for_ts)\n",
    "    df_abnormal_preprocessed = preprocessing_dataframe(df_abnormal,desired_length_for_ts)\n",
    "\n",
    "    df_classifier = (pd.concat([df_normal_preprocessed,df_abnormal_preprocessed]).reset_index()).drop(columns='index')\n",
    "    acc_mean,acc_std_dev,prec_mean,prec_std_dev,recall_mean,recall_std_dev,f1_mean,f1_std_dev = ts_classifier(df_classifier)\n",
    "\n",
    "    classifier_dict_healthy[key] = {'acc_mean': acc_mean,'acc_std_dev': acc_std_dev,'prec_mean': prec_mean,'prec_std_dev': prec_std_dev,'recall_mean': recall_mean,'recall_std_dev': recall_std_dev,'f1_mean': f1_mean,'f1_std_dev': f1_std_dev}\n",
    "\n",
    "else:\n",
    "    df_abnormal = final_abnormal_df\n",
    "\n",
    "    random.seed(42)\n",
    "    random_sample_numbers = random.sample(normal_sample_numbers, abnormal_no)\n",
    "    df_normal = final_normal_df[final_normal_df['Subject'].isin(random_sample_numbers)]\n",
    "    display(df_normal)\n",
    "\n",
    "    desired_length_for_ts = time_series_length_finding(df_abnormal)\n",
    "\n",
    "    df_normal_preprocessed = preprocessing_normal_df(df_normal,desired_length_for_ts)\n",
    "    df_abnormal_preprocessed = preprocessing_dataframe(df_abnormal,desired_length_for_ts)\n",
    "\n",
    "    df_classifier = (pd.concat([df_normal_preprocessed,df_abnormal_preprocessed]).reset_index()).drop(columns='index')\n",
    "    acc_mean,acc_std_dev,prec_mean,prec_std_dev,recall_mean,recall_std_dev,f1_mean,f1_std_dev = ts_classifier(df_classifier)\n",
    "\n",
    "    classifier_dict_healthy[key] = {'acc_mean': acc_mean,'acc_std_dev': acc_std_dev,'prec_mean': prec_mean,'prec_std_dev': prec_std_dev,'recall_mean': recall_mean,'recall_std_dev': recall_std_dev,'f1_mean': f1_mean,'f1_std_dev': f1_std_dev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16228865",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dict_healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f3ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the file path where you want to save the pickle file\n",
    "pickle_file_path = 'healthy_EMO_pain_MUSE_normalized.pickle'\n",
    "\n",
    "# Save the dictionary to a pickle file\n",
    "with open(pickle_file_path, 'wb') as file:\n",
    "    pickle.dump(classifier_dict_healthy, file)\n",
    "\n",
    "print(f\"Dictionary saved to {pickle_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
